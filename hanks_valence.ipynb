{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Objective: Given a **verb**, a **valence** for the verb and a **corpus**, find the clusters of semantic type that fill the valence of the verb and return a probability distribution over the clusters.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Load the sub-obj pairs\n",
    "2. Load the semantic types inventory\n",
    "3. For each word (subject or object) in the sub-obj pairs:\n",
    "    1. Retrieve the synset of the subject and object\n",
    "    2. Retrieve the semantic type of the synset\n",
    "    3. Add the semantic type to the list of semantic types for the word\n",
    "4. Create objects that hold sub-obj pairs and their number of occurrences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0b04906eb92af7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from typing import Tuple, Dict, Set, List\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:27:30.430560Z",
     "start_time": "2024-04-10T10:27:30.419015Z"
    }
   },
   "id": "3183bffc4932d2e",
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load the sub-obj pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b9e08ca8106897a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Subject       Object\n0  Republicans           it\n1          You        signs\n2    Democrats  opportunity\n3        which     soldiers\n4         they     approach",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Subject</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Republicans</td>\n      <td>it</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You</td>\n      <td>signs</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Democrats</td>\n      <td>opportunity</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>which</td>\n      <td>soldiers</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>they</td>\n      <td>approach</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open dataframe\n",
    "df = pd.read_csv('res/see_sub_obj_pairs.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:27:30.461450Z",
     "start_time": "2024-04-10T10:27:30.440516Z"
    }
   },
   "id": "b699725054e5b08d",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Load the semantic types inventory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f08b2992df6290e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8641944, 'n') GEOGRAPHY_AND_PLACES_\n",
      "(8950407, 'n') GEOGRAPHY_AND_PLACES_\n",
      "(4502851, 'n') WARFARE_DEFENSE_AND_VIOLENCE_\n",
      "(13742358, 'n') MATHEMATICS_\n",
      "(13742573, 'n') MATHEMATICS_\n",
      "(14930670, 'n') CHEMISTRY_AND_MINERALOGY_\n",
      "(475142, 'n') SPORT_GAMES_AND_RECREATION_\n",
      "(13746512, 'n') MATHEMATICS_\n",
      "(13750415, 'n') MATHEMATICS_\n",
      "(13750844, 'n') MATHEMATICS_\n",
      "(13751265, 'n') MATHEMATICS_\n"
     ]
    }
   ],
   "source": [
    "# holds a mapping (wordnet id, wordnet pos) -> semantic type\n",
    "wn_semantic_type_dict: Dict[Tuple[int, str], str] = {}\n",
    "\n",
    "# open semantic types inventory\n",
    "with open('res/st_inventory.txt', 'r') as f:\n",
    "    # shorter version\n",
    "    # wn_semantic_type_dict = {line.split('\\t')[0]: line.split('\\t')[1] for line in f.read().splitlines()[:10]}\n",
    "    \n",
    "    st_inventory = f.read().splitlines()\n",
    "    # Each line is like this: wn:08641944n\\tGEOGRAPHY_AND_PLACES_\n",
    "    for line in st_inventory:\n",
    "        l = line.split('\\t')\n",
    "        wn_id = l[0]\n",
    "        # separate id in the form wn:08641944n to [08641944, n]\n",
    "        wn_id_number, wn_id_pos = int(wn_id[3:-1]), wn_id[-1]\n",
    "        semantic_type = l[1]\n",
    "        wn_semantic_type_dict[(wn_id_number, wn_id_pos)] = semantic_type\n",
    "        \n",
    "# print first 10 entries\n",
    "for i, (wn_id, st) in enumerate(wn_semantic_type_dict.items()):\n",
    "    print(wn_id, st)\n",
    "    if i == 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:28:51.396094Z",
     "start_time": "2024-04-10T10:28:51.282308Z"
    }
   },
   "id": "c0345bf88a7bd8ee",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(''hood.n.01')\n",
      "Synset('the_hague.n.01')\n",
      "Synset('twenty-two.n.02')\n",
      "Synset('zero.n.02')\n",
      "Synset('one.n.01')\n",
      "Synset('lauryl_alcohol.n.01')\n",
      "Synset('one-hitter.n.01')\n",
      "Synset('ten.n.01')\n",
      "Synset('hundred.n.01')\n",
      "Synset('thousand.n.01')\n"
     ]
    }
   ],
   "source": [
    "# retrieve wordnet synset from wordnet id\n",
    "def get_synset_from_id(wn_id: Tuple[int, str]) -> Synset:\n",
    "    \"\"\"\n",
    "    Given a wordnet id (id, pos), return the corresponding synset\n",
    "    \"\"\"\n",
    "    return wn.synset_from_pos_and_offset(wn_id[1], wn_id[0])\n",
    "\n",
    "# print first 10 synsets\n",
    "for wn_id in list(wn_semantic_type_dict.keys())[:10]:\n",
    "    print(get_synset_from_id(wn_id))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:27:30.647877Z",
     "start_time": "2024-04-10T10:27:30.635049Z"
    }
   },
   "id": "b87454d41f7374c0",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save list of synsets available\n",
    "synset_list: List[Synset] = []\n",
    "\n",
    "for wn_id in list(wn_semantic_type_dict.keys()):\n",
    "    synset_list.append(get_synset_from_id(wn_id))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb6fe5a3658fa356"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for each word in the sub-obj pairs, find all their synsets\n",
    "word_synset_dict = {}\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def add_word_synsets(word, word_synset_dict):\n",
    "    \"\"\"\n",
    "    Given a word, retrieve all the synsets from st_inventory where the word is included, and add them to the word_synset_dict\n",
    "    \"\"\"\n",
    "    # Lemmatize and retrieve synsets if not already done\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    if lemmatized_word not in word_synset_dict: # if the word is not already in the dictionary\n",
    "        word_synset_dict[lemmatized_word] = []\n",
    "        for synset in synset_list: # iterate synsets in the st_inventory\n",
    "            if lemmatized_word in synset.lemma_names(): # if the word is in the synset\n",
    "                word_synset_dict[lemmatized_word].append(synset) # add the synset to the list of synsets for the word\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    add_word_synsets(row['subject'], word_synset_dict)\n",
    "    add_word_synsets(row['object'], word_synset_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2afc438df8921"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec2c4a1cec02de6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
